{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPV18q0RPoURMTwlUK1iHZx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/umair986/salah_detection/blob/main/salah_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9Bs10DCpZpm5"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the MoveNet model\n",
        "interpreter = tf.lite.Interpreter(model_path='/content/3.tflite')\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Define edges for visualization\n",
        "EDGES = {\n",
        "    (0, 1): 'm', (0, 2): 'c', (1, 3): 'm', (2, 4): 'c',\n",
        "    (0, 5): 'm', (0, 6): 'c', (5, 7): 'm', (7, 9): 'm',\n",
        "    (6, 8): 'c', (8, 10): 'c', (5, 6): 'y', (5, 11): 'm',\n",
        "    (6, 12): 'c', (11, 12): 'y', (11, 13): 'm', (13, 15): 'm',\n",
        "    (12, 14): 'c', (14, 16): 'c'\n",
        "}\n"
      ],
      "metadata": {
        "id": "9Hi7GZNtaIly"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess the input frame\n",
        "def preprocess_frame(frame):\n",
        "    img = tf.image.resize_with_pad(np.expand_dims(frame, axis=0), 192, 192)\n",
        "    return tf.cast(img, dtype=tf.float32)\n",
        "\n",
        "# Draw keypoints on the frame\n",
        "def draw_keypoints(frame, keypoints, confidence_threshold):\n",
        "    y, x, _ = frame.shape\n",
        "    keypoints = np.squeeze(np.multiply(keypoints, [y, x, 1]))\n",
        "\n",
        "    for kp in keypoints:\n",
        "        ky, kx, kp_conf = kp\n",
        "        if kp_conf > confidence_threshold:\n",
        "            cv2.circle(frame, (int(kx), int(ky)), 4, (0, 255, 0), -1)\n"
      ],
      "metadata": {
        "id": "3wz29mG9aVKY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Draw edges on the frame\n",
        "def draw_connections(frame, keypoints, edges, confidence_threshold):\n",
        "    y, x, _ = frame.shape\n",
        "    keypoints = np.squeeze(np.multiply(keypoints, [y, x, 1]))\n",
        "\n",
        "    for edge, color in edges.items():\n",
        "        p1, p2 = edge\n",
        "        y1, x1, c1 = keypoints[p1]\n",
        "        y2, x2, c2 = keypoints[p2]\n",
        "\n",
        "        if c1 > confidence_threshold and c2 > confidence_threshold:\n",
        "            cv2.line(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 0, 255), 2)\n"
      ],
      "metadata": {
        "id": "zgG0bH8waZ-B"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to classify postures\n",
        "def classify_posture(keypoints):\n",
        "    keypoints = np.squeeze(keypoints)\n",
        "\n",
        "    # Extract relevant keypoints\n",
        "    nose = keypoints[0]\n",
        "    left_shoulder = keypoints[5]\n",
        "    right_shoulder = keypoints[6]\n",
        "    left_elbow = keypoints[7]\n",
        "    right_elbow = keypoints[8]\n",
        "    left_wrist = keypoints[9]\n",
        "    right_wrist = keypoints[10]\n",
        "    left_hip = keypoints[11]\n",
        "    right_hip = keypoints[12]\n",
        "    left_knee = keypoints[13]\n",
        "    right_knee = keypoints[14]\n",
        "    left_ankle = keypoints[15]\n",
        "    right_ankle = keypoints[16]\n",
        "\n",
        "\n",
        "    # Qiyaam (Standing straight)\n",
        "    if (\n",
        "        nose[2] > 0.5 and left_shoulder[2] > 0.5 and right_shoulder[2] > 0.5 and\n",
        "        left_hip[2] > 0.5 and right_hip[2] > 0.5 and\n",
        "        left_knee[2] > 0.5 and right_knee[2] > 0.5\n",
        "    ):\n",
        "        if left_elbow[1] < left_shoulder[1] and right_elbow[1] > right_shoulder[1]:\n",
        "            return \"Qiyaam\"\n",
        "\n",
        "    # Rukoo (Bowing)\n",
        "    if (\n",
        "        left_shoulder[1] < left_hip[1] and right_shoulder[1] < right_hip[1] and\n",
        "        abs(left_shoulder[0] - left_hip[0]) < 0.1\n",
        "    ):\n",
        "        return \"Rukoo\"\n",
        "\n",
        "    # Sujood (Prostration)\n",
        "    if (\n",
        "        left_knee[2] > 0.5 and right_knee[2] > 0.5 and\n",
        "        left_ankle[2] > 0.5 and right_ankle[2] > 0.5 and\n",
        "        left_wrist[2] > 0.5 and right_wrist[2] > 0.5\n",
        "    ):\n",
        "        return \"Sujood\"\n",
        "\n",
        "    # Qa'dah (Sitting)\n",
        "    if (\n",
        "        left_knee[2] > 0.5 and right_knee[2] > 0.5 and\n",
        "        left_hip[2] > 0.5 and right_hip[2] > 0.5 and\n",
        "        abs(left_shoulder[0] - left_hip[0]) > 0.1\n",
        "    ):\n",
        "        return \"Qa'dah\"\n",
        "            # Salaam (Turning head to shoulders)\n",
        "    if nose[2] > 0.5 and (\n",
        "        abs(nose[1] - right_shoulder[1]) < 0.1 or abs(nose[1] - left_shoulder[1]) < 0.1\n",
        "    ):\n",
        "        return \"Salaam\"\n",
        "\n",
        "    return \"Unknown Posture\"\n"
      ],
      "metadata": {
        "id": "cWz-fC_cacPK"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main loop for video processing\n",
        "cap = cv2.VideoCapture(0)\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Preprocess the frame\n",
        "    input_image = preprocess_frame(frame)\n",
        "     # Model inference\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "\n",
        "    interpreter.set_tensor(input_details[0]['index'], np.array(input_image))\n",
        "    interpreter.invoke()\n",
        "    keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "    # Classify the posture\n",
        "    posture = classify_posture(keypoints_with_scores)\n",
        "    print(f\"Detected Posture: {posture}\")\n",
        "       # Draw keypoints and connections\n",
        "    draw_connections(frame, keypoints_with_scores, EDGES, 0.4)\n",
        "    draw_keypoints(frame, keypoints_with_scores, 0.4)\n",
        "\n",
        "    # Display posture on the frame\n",
        "    cv2.putText(frame, f\"Posture: {posture}\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
        "\n",
        "    # Show the frame\n",
        "    cv2.imshow('Namaz Posture Detector', frame)\n",
        "\n",
        "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "BTHn2UkGafI-"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "from collections import deque"
      ],
      "metadata": {
        "id": "o7j_P6Z0bNCZ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the MoveNet model\n",
        "interpreter = tf.lite.Interpreter(model_path='/content/4.tflite')\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Define edges for visualization\n",
        "EDGES = {\n",
        "    (0, 1): 'm', (0, 2): 'c', (1, 3): 'm', (2, 4): 'c',\n",
        "    (0, 5): 'm', (0, 6): 'c', (5, 7): 'm', (7, 9): 'm',\n",
        "    (6, 8): 'c', (8, 10): 'c', (5, 6): 'y', (5, 11): 'm',\n",
        "    (6, 12): 'c', (11, 12): 'y', (11, 13): 'm', (13, 15): 'm',\n",
        "    (12, 14): 'c', (14, 16): 'c'\n",
        "}"
      ],
      "metadata": {
        "id": "B0TMOd6Nbm4y"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Confidence threshold and smoothing window\n",
        "CONFIDENCE_THRESHOLD = 0.6\n",
        "SMOOTHING_WINDOW = 5\n",
        "keypoints_buffer = deque(maxlen=SMOOTHING_WINDOW)\n",
        "\n",
        "# Posture sequence and validation\n",
        "POSTURE_SEQUENCE = [\"Qiyaam\", \"Rukoo\", \"Sujood\", \"Qa'dah\", \"Salaam\"]\n",
        "current_index = 0"
      ],
      "metadata": {
        "id": "ZBlqgkK4bqsE"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_frame(frame):\n",
        "    \"\"\"Preprocess the input frame for the model.\"\"\"\n",
        "    img = tf.image.resize_with_pad(np.expand_dims(frame, axis=0), 256, 256)  # Resize to 256x256\n",
        "    return tf.cast(img, dtype=tf.uint8)  # Ensure dtype matches model input requirement"
      ],
      "metadata": {
        "id": "lxOq9nvDdZLm"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_keypoints(frame, keypoints, confidence_threshold):\n",
        "    \"\"\"Draw keypoints on the frame.\"\"\"\n",
        "    y, x, _ = frame.shape\n",
        "    keypoints = np.squeeze(np.multiply(keypoints, [y, x, 1]))\n",
        "\n",
        "    for kp in keypoints:\n",
        "        ky, kx, kp_conf = kp\n",
        "        if kp_conf > confidence_threshold:\n",
        "            cv2.circle(frame, (int(kx), int(ky)), 4, (0, 255, 0), -1)\n"
      ],
      "metadata": {
        "id": "DeAvt1FRdbrT"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_connections(frame, keypoints, edges, confidence_threshold):\n",
        "    \"\"\"Draw edges between keypoints on the frame.\"\"\"\n",
        "    y, x, _ = frame.shape\n",
        "    keypoints = np.squeeze(np.multiply(keypoints, [y, x, 1]))\n",
        "\n",
        "    for edge, color in edges.items():\n",
        "        p1, p2 = edge\n",
        "        y1, x1, c1 = keypoints[p1]\n",
        "        y2, x2, c2 = keypoints[p2]\n",
        "\n",
        "        if c1 > confidence_threshold and c2 > confidence_threshold:\n",
        "            cv2.line(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 0, 255), 2)\n",
        "\n"
      ],
      "metadata": {
        "id": "Ih-ycZImdgfs"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_angle(p1, p2, p3):\n",
        "    \"\"\"Calculate the angle between three points.\"\"\"\n",
        "    v1 = np.array(p1) - np.array(p2)\n",
        "    v2 = np.array(p3) - np.array(p2)\n",
        "    angle = np.arccos(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))\n",
        "    return np.degrees(angle)"
      ],
      "metadata": {
        "id": "sNxRaP1cdg7w"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_posture(keypoints):\n",
        "    \"\"\"Classify the posture based on keypoints.\"\"\"\n",
        "    keypoints = np.squeeze(keypoints)\n",
        "\n",
        "    # Extract relevant keypoints\n",
        "    nose = keypoints[0]\n",
        "    left_shoulder = keypoints[5]\n",
        "    right_shoulder = keypoints[6]\n",
        "    left_elbow = keypoints[7]\n",
        "    right_elbow = keypoints[8]\n",
        "    left_hip = keypoints[11]\n",
        "    right_hip = keypoints[12]\n",
        "    left_knee = keypoints[13]\n",
        "    right_knee = keypoints[14]\n",
        "    left_ankle = keypoints[15]\n",
        "    right_ankle = keypoints[16]\n",
        "\n",
        "    # Qiyaam (Standing straight with arms folded)\n",
        "    if (\n",
        "        nose[2] > CONFIDENCE_THRESHOLD and left_shoulder[2] > CONFIDENCE_THRESHOLD and right_shoulder[2] > CONFIDENCE_THRESHOLD and\n",
        "        left_hip[2] > CONFIDENCE_THRESHOLD and right_hip[2] > CONFIDENCE_THRESHOLD and\n",
        "        left_knee[2] > CONFIDENCE_THRESHOLD and right_knee[2] > CONFIDENCE_THRESHOLD\n",
        "    ):\n",
        "        if left_elbow[1] < left_shoulder[1] and right_elbow[1] > right_shoulder[1]:\n",
        "            return \"Qiyaam\"\n",
        "\n",
        "    # Rukoo (Bowing with back parallel to the ground)\n",
        "    if (\n",
        "        left_shoulder[1] < left_hip[1] and right_shoulder[1] < right_hip[1] and\n",
        "        abs(left_shoulder[0] - left_hip[0]) < 0.1\n",
        "    ):\n",
        "        back_angle = calculate_angle(left_hip[:2], left_shoulder[:2], left_knee[:2])\n",
        "        if 80 <= back_angle <= 100:\n",
        "            return \"Rukoo\"\n",
        "\n",
        "    # Sujood (Prostration)\n",
        "    if (\n",
        "        left_knee[2] > CONFIDENCE_THRESHOLD and right_knee[2] > CONFIDENCE_THRESHOLD and\n",
        "        left_ankle[2] > CONFIDENCE_THRESHOLD and right_ankle[2] > CONFIDENCE_THRESHOLD and\n",
        "        left_elbow[2] > CONFIDENCE_THRESHOLD and right_elbow[2] > CONFIDENCE_THRESHOLD\n",
        "    ):\n",
        "        return \"Sujood\"\n",
        "\n",
        "    # Qa'dah (Sitting on knees with straight back)\n",
        "    if (\n",
        "        left_knee[2] > CONFIDENCE_THRESHOLD and right_knee[2] > CONFIDENCE_THRESHOLD and\n",
        "        left_hip[2] > CONFIDENCE_THRESHOLD and right_hip[2] > CONFIDENCE_THRESHOLD and\n",
        "        abs(left_shoulder[0] - left_hip[0]) > 0.1\n",
        "    ):\n",
        "        return \"Qa'dah\"\n",
        "\n",
        "    # Salaam (Turning head towards shoulders)\n",
        "    if nose[2] > CONFIDENCE_THRESHOLD and (\n",
        "        abs(nose[1] - right_shoulder[1]) < 0.1 or abs(nose[1] - left_shoulder[1]) < 0.1\n",
        "    ):\n",
        "        return \"Salaam\"\n",
        "\n",
        "    return \"Unknown Posture\""
      ],
      "metadata": {
        "id": "02emPM_xdiYi"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def smooth_keypoints(new_keypoints):\n",
        "    \"\"\"Smooth keypoints using a rolling average.\"\"\"\n",
        "    keypoints_buffer.append(new_keypoints)\n",
        "    return np.mean(keypoints_buffer, axis=0)"
      ],
      "metadata": {
        "id": "Ygjrg3UjdlwK"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_posture_sequence(detected_posture):\n",
        "    \"\"\"Validate the sequence of postures.\"\"\"\n",
        "    global current_index\n",
        "    if detected_posture == POSTURE_SEQUENCE[current_index]:\n",
        "        current_index += 1\n",
        "        if current_index == len(POSTURE_SEQUENCE):\n",
        "            print(\"Namaz completed properly!\")\n",
        "            current_index = 0  # Reset for the next cycle\n",
        "    elif detected_posture != \"Unknown Posture\":\n",
        "        print(\"Invalid posture sequence detected.\")\n"
      ],
      "metadata": {
        "id": "Hya0qcwZdnnP"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main loop for video processing\n",
        "cap = cv2.VideoCapture(0)\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Preprocess the frame\n",
        "    input_image = preprocess_frame(frame)\n",
        "\n",
        "    # Model inference\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "\n",
        "    interpreter.set_tensor(input_details[0]['index'], np.array(input_image))\n",
        "    interpreter.invoke()\n",
        "    keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "    # Smooth keypoints and classify posture\n",
        "    smoothed_keypoints = smooth_keypoints(keypoints_with_scores)\n",
        "    posture = classify_posture(smoothed_keypoints)\n",
        "    validate_posture_sequence(posture)\n",
        "\n",
        "    # Draw keypoints and connections\n",
        "    draw_connections(frame, smoothed_keypoints, EDGES, CONFIDENCE_THRESHOLD)\n",
        "    draw_keypoints(frame, smoothed_keypoints, CONFIDENCE_THRESHOLD)\n",
        "\n",
        "    # Display posture on the frame\n",
        "    cv2.putText(frame, f\"Posture: {posture}\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
        "\n",
        "    # Show the frame\n",
        "    cv2.imshow('Namaz Posture Detector', frame)\n",
        "\n",
        "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "V0E-TUFwdpH1"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DF1oUOvhds9q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}